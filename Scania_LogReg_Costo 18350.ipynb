{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scania_Failures  Mediante Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data Set/aps_failure_training_set.csv',na_values='na')\n",
    "df_test = pd.read_csv('Data Set/aps_failure_test_set.csv', na_values='na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_train.drop(columns=['class']).columns.values\n",
    "df_train[feature_columns]=df_train[feature_columns].fillna(df_train.mean())\n",
    "df_test[feature_columns]=df_test[feature_columns].fillna(df_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train target:\n",
      "0    59000\n",
      "1     1000\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'] = df_train['class'].replace ({'neg':0, 'pos':1})\n",
    "df_test['target'] = df_test['class'].replace ({'neg':0, 'pos':1})\n",
    "print( 'df_train target:'), print(df_train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_train[feature_columns]\n",
    "y = df_train['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, class_weight={0: 1, 1: 50}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "modelo = LogisticRegression(C=5, class_weight={0:1, 1:50})\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones correctas :  5841\n",
      "Número de muestras     :  6000\n",
      "Exactitud (manual)     :  0.9735\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicciones_val = modelo.predict(X_val)\n",
    "num_predicciones_correctas = (y_val == predicciones_val).sum()\n",
    "num_total_de_muestras = len(y_val)\n",
    "exactitud = num_predicciones_correctas / num_total_de_muestras\n",
    "\n",
    "print ( 'Predicciones correctas : ', num_predicciones_correctas )\n",
    "print ( 'Número de muestras     : ', num_total_de_muestras )\n",
    "print ( 'Exactitud (manual)     : ', exactitud )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def print_binary_confusion_matrix(matrix):    \n",
    "    TN = matrix[0,0]\n",
    "    FN = matrix[1,0]\n",
    "    FP = matrix[0,1]\n",
    "    TP = matrix[1,1]\n",
    "\n",
    "    print ('              +-----------------+')\n",
    "    print ('              |   Predicción    |')\n",
    "    print ('              +-----------------+')\n",
    "    print ('              |    +   |    -   |')\n",
    "    print ('+-------+-----+--------+--------+')\n",
    "    print ('| Valor |  +  |  {:5d} |  {:5d} |'.format(TP, FN) )\n",
    "    print ('| real  +-----+--------+--------+')\n",
    "    print ('|       |  -  |  {:5d} |  {:5d} |'.format(FP, TN) )\n",
    "    print ('+-------+-----+--------+--------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------------+\n",
      "              |   Predicción    |\n",
      "              +-----------------+\n",
      "              |    +   |    -   |\n",
      "+-------+-----+--------+--------+\n",
      "| Valor |  +  |     88 |      7 |\n",
      "| real  +-----+--------+--------+\n",
      "|       |  -  |    152 |   5753 |\n",
      "+-------+-----+--------+--------+\n",
      "Exactitud en el conjunto de validación     : 0.974\n",
      "Precisión en el conjunto de validación     : 0.367\n",
      "Exhaustividad en el conjunto de validación : 0.926\n"
     ]
    }
   ],
   "source": [
    "predecido = predicciones_val\n",
    "matriz = confusion_matrix (y_val, predecido)\n",
    "\n",
    "accuracy_val = metrics.accuracy_score(y_val, predecido)\n",
    "precision_val = metrics.precision_score (y_val, predecido)\n",
    "recall_val = metrics.recall_score (y_val, predecido)\n",
    "\n",
    "print_binary_confusion_matrix(matriz)\n",
    "print('Exactitud en el conjunto de validación     : {:.3f}'.format(accuracy_val))\n",
    "print('Precisión en el conjunto de validación     : {:.3f}'.format(precision_val))\n",
    "print('Exhaustividad en el conjunto de validación : {:.3f}'.format(recall_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones correctas :  15583\n",
      "Número de muestras     :  16000\n",
      "Exactitud (metrics)    :  0.9739375\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test[feature_columns]\n",
    "y_test = df_test['target']\n",
    "\n",
    "# De clase\n",
    "\n",
    "predicciones_test = modelo.predict(X_test)\n",
    "num_predicciones_correctas = (y_test == predicciones_test).sum()\n",
    "num_total_de_muestras = len(y_test)\n",
    "exactitud = num_predicciones_correctas / num_total_de_muestras\n",
    "\n",
    "print ( 'Predicciones correctas : ', num_predicciones_correctas )\n",
    "print ( 'Número de muestras     : ', num_total_de_muestras )\n",
    "print ( 'Exactitud (metrics)    : ', metrics.accuracy_score(y_test, predicciones_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De clase\n",
    "from sklearn import metrics\n",
    "accuracy_val = metrics.accuracy_score(y_val, predecido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De clase\n",
    "def print_binary_confusion_matrix(matrix):    \n",
    "    TN = matrix[0,0]\n",
    "    FN = matrix[1,0]\n",
    "    FP = matrix[0,1]\n",
    "    TP = matrix[1,1]\n",
    "\n",
    "    print ('              +-----------------+')\n",
    "    print ('              |   Predicción    |')\n",
    "    print ('              +-----------------+')\n",
    "    print ('              |    +   |    -   |')\n",
    "    print ('+-------+-----+--------+--------+')\n",
    "    print ('| Valor |  +  |  {:5d} |  {:5d} |'.format(TP, FN) )\n",
    "    print ('| real  +-----+--------+--------+')\n",
    "    print ('|       |  -  |  {:5d} |  {:5d} |'.format(FP, TN) )\n",
    "    print ('+-------+-----+--------+--------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------------+\n",
      "              |   Predicción    |\n",
      "              +-----------------+\n",
      "              |    +   |    -   |\n",
      "+-------+-----+--------+--------+\n",
      "| Valor |  +  |    346 |     29 |\n",
      "| real  +-----+--------+--------+\n",
      "|       |  -  |    388 |  15237 |\n",
      "+-------+-----+--------+--------+\n",
      "Exactitud en el conjunto de validación     : 0.974\n",
      "Precisión en el conjunto de validación     : 0.471\n",
      "Exhaustividad en el conjunto de validación : 0.923\n"
     ]
    }
   ],
   "source": [
    "predecido = predicciones_test\n",
    "\n",
    "matriz = confusion_matrix (y_test, predecido)\n",
    "\n",
    "accuracy_val = metrics.accuracy_score(y_test, predecido)\n",
    "precision_val = metrics.precision_score (y_test, predecido)\n",
    "recall_val = metrics.recall_score (y_test, predecido)\n",
    "\n",
    "print_binary_confusion_matrix(matriz)\n",
    "print('Exactitud en el conjunto de validación     : {:.3f}'.format(accuracy_val))\n",
    "print('Precisión en el conjunto de validación     : {:.3f}'.format(precision_val))\n",
    "print('Exhaustividad en el conjunto de validación : {:.3f}'.format(recall_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:  18380\n"
     ]
    }
   ],
   "source": [
    "#Costo de Pronostico (Santosh)\n",
    "\n",
    "U_check = 10 #cost that an unnecessary check\n",
    "M_check = 500 # cost of missing a faulty truck\n",
    "\n",
    "FP = ((predicciones_test > 0) & (y_test == 0)).sum()\n",
    "FN = ((predicciones_test <= 0) & (y_test == 1)).sum()\n",
    "\n",
    "Total_Cost = FP*U_check + FN*M_check\n",
    "print('Total Cost: ', Total_Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
